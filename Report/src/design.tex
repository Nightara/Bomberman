To tackle the variety of different tasks our agent has to solve, we decided to split our agent into separate subtasks,
each of which can be trained individually without affecting the performance of the other tasks.
To combine all agents into a single agent capable of handling all tasks at once, we created a governor agent, whose only
task is to handle the inputs of all subtasks, compare them, and pick the action provided by the agent with the highest
confidence in its prediction.
To enable this functionality, we extended the original agent structure with an additional \listinline{next\_move()}
method, which takes the exact same input as \listinline{act()}, but returns a tuple of the next move and a confidence
value within [0, 1].
The governor then takes these move suggestions, weights them based on an internal decision system, and returns one of
the provided suggestions as its overall choice.

The  advantage of this approach is the ability to tweak the performance at one task without disrupting any other task -
either by improving the corresponding sub-agent, or by adjusting the weights to accommodate for a hyperfocus on certain
tasks.

For our submission, we decided to split our agent into the following sub-agents:
\begin{itemize}
  \item Survivor: An agent focusing on not getting hit by explosions.
  \item Collector: An agent tasked with pathing to and collecting coins.
  \item Coin Creator: An agent trying to maximize the number of crates destroyed to generate new coins and paths.
  \item Pathfinder: An agent focusing on tracking down opponents.
  \item Bomber: An agent with the goal to take down opponents.
\end{itemize}

\subsection{Governor Design}
\subsubsection*{Florian}
\label{subsec:design-governor}
In the submitted version, we decided to go with a simple, static implementation of the governor.
Every sub-agent is registered within the governor by providing its \listinline{setup()} and \listinline{next\_move()}
callbacks to the governor, which are then stored for later.
During its own \listinline{setup()}, the governor forwards the call to all sub-agents, and then initializes the
sub-agent weights for the decision algorithm.
In its \listinline{act()} method, it once again forwards the call to all sub-agents, retrieves the suggestions and
confidence values for each of them, and picks the move associated with the highest confidence value after multiplication
with its stored weights.
As there are up to six different moves an agent can return, our general goal was to aim for a confidence significantly
higher than 0.16 for our sub-agents to differentiate between a random low-confidence and a deliberate high-confidence
choice.
The weights of all sub-agents were determined experimentally, and the last part of our \enquote{training} consisted of
testing multiple variations, tweaking these weights whenever we saw the governor focusing too much on one aspect of the
game.

\subsection{Survivor Design}
\subsubsection*{Florian}
\label{subsec:survivor-design}
The task we considered to be the most important one is the Survivor agent.
After some thought, we decided to only focus on short-term survival, which allows this agent to only use what we called
\enquote{local vision}, or vision of the 5x5 square centered around the agent.
Since this limited input greatly reduces the complexity of the agent, we decided to use a three-layer neural network
with 25 input neurons (Each of them corresponding to one tile of the local square), one hidden layer, and five output
neurons, each of which encodes for one of the five possible actions (WAIT, UP, DOWN, LEFT, RIGHT).

Like every agent, the survivor also has to provide confidence for its prediction.
The goal was to have high confidence (> 0.33, as there can be up to three different ways to escape an explosion)
whenever the survivor is too close to an explosion, and a low confidence (< 0.2, as the agent has five different moves
to choose from) if there is no immediate threat to our agent.

Fortunately, softmax and similar encodings already provide us with a bounded value in the exact range of our confidence
values for every action, which made neural networks a great choice for the overall agent design we had in mind.

For the actual implementation of this sub-agent, we decided to use the pytorch library, which we had already been
familiarized with during the lecture.

% TODO: Insert one subsection for every other agent here.
\subsection{Bombing Design}
\subsubsection*{Keerthan Ugrani}
\label{subsec:survivor-design}
This task of the player is the deciding factor in winning the game. The bombing agent has a global vision of the game state, which means that this agent has information about the positions of the crates and the positions of the enemy players. The bombing agent has been programmed logically to strategically place the bomb in the game environment, this could include the case that the agent finds the dead end and finds out either of the obstacles is a crate. We also make this agent more sophisticated by effectively calculating the effectiveness of the bomb and taking steps to avoid getting caught in the explosion of the self-placed bombs. 

The bombing agent provides confidence according to the score for all the possible moves, which means that the bombing agent returns the moves and the respective confidence of each move just like every other agent. Here it depends on the Q-value of each action taken into consideration and the move with the highest Q-value is returned.

Q-learning is a foundational reinforcement learning algorithm used in machine learning to find the optimal action-selection policy for a given finite Markov decision process. The algorithm iteratively refines estimates of action values, allowing an agent to make informed decisions in an environment. The Q-Learning update equation is a crucial component of this algorithm, determining how the agent updates its knowledge based on experiences.

The Q-Learning update equation is as follows:

\[Q(s, a) \leftarrow (1 - \alpha) \cdot Q(s, a) + \alpha \cdot \left(r + \gamma \cdot \max_{a'} Q(s', a')\right)\]

The update equation combines the current estimate Q(s, a) with new information obtained during an agent's interaction with the environment.

The Q-learning functions collectively form a comprehensive toolkit for training and evaluating an agent in a game environment. By addressing reward assignment, policy generation, experience accumulation, and neural network updates, these functions contribute to the agent's ability to learn effective strategies for gameplay.

\subsection{Second Agent}
\label{subsec:second-design}
As required by the rules laid out in chapter~\ref{subsec:rules}, we had to submit at least two different agents for the
project.
Our governor design technically already includes multiple sub-agents, but none of them would be capable of properly
playing (And winning) the game against somewhat skilled opponents on its own.
Because the q-learning algorithm used for the bombing agent achieved fairly good results, we decided to train an entire
bot using only q-learning for our second submission.
% TODO: Add more information
